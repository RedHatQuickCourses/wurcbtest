#  Troubleshoot containers and pods

= Troubleshoot Containers and Pods
:navtitle: Troubleshoot Containers and Pods
:page-aliases: troubleshoot-containers-pods.adoc

This module provides an in-depth guide to troubleshooting common issues with containers and pods in Azure Red Hat OpenShift (ARO) environments. Effective troubleshooting is a critical skill for managing applications on Kubernetes, enabling you to diagnose problems, understand application behavior, and ensure operational stability.

[[pod-lifecycle-states]]
== Understanding Pod Lifecycle and States

Pods in Kubernetes (and OpenShift) transition through various phases, indicating their current state. Understanding these states is the first step in diagnosing issues.

A pod's lifecycle is primarily defined by its `status.phase` and the states of its individual containers. Key pod phases include:

*   *Pending*: The pod has been accepted by the Kubernetes cluster but one or more of the container images have not been created. This could be due to a lack of available resources (CPU, memory), image pull errors, or pending volume attachments.
*   *Running*: The pod has been bound to a node, and all of its containers have been created. At least one container is still running, or is in the process of starting or restarting.
*   *Succeeded*: All containers in the pod have terminated successfully, and will not be restarted. This phase is typical for batch jobs or one-off tasks.
*   *Failed*: All containers in the pod have terminated, and at least one container has terminated in failure (i.e., it exited with a non-zero exit code or was terminated by the system).
*   *Unknown*: For some reason, the state of the pod could not be obtained. This usually indicates an issue with the Kubernetes control plane or network communication.

Within a pod, individual containers also have their own states: *Waiting* (e.g., ImagePullBackOff, ContainerCreating), *Running*, and *Terminated*. Common reasons for containers to be in a Waiting or Terminated state include:

*   `ImagePullBackOff`: The image specified for the container could not be pulled (e.g., incorrect image name, private registry access issues).
*   `CrashLoopBackOff`: The container started, crashed, and is continually restarting. This often points to an application error within the container.
*   `OOMKilled`: The container was terminated by the operating system due to exceeding its memory limits.

[[diagnosing-pod-issues]]
== Diagnosing Pod Issues

When a pod is not behaving as expected (e.g., stuck in Pending, continually crashing), you can use a series of `oc` (OpenShift CLI) commands to gather information.

=== 1. Checking Pod Status

The most basic step is to quickly check the current status of your pods.

*   To list all pods in the current project (namespace):
    ```bash
    oc get pods
    ```
*   To list pods and their current states, including restart counts and node assignment:
    ```bash
    oc get pods -o wide
    ```
    Pay close attention to the `STATUS` column (e.g., `Running`, `Pending`, `CrashLoopBackOff`) and the `RESTARTS` count. A high restart count often indicates a crashing container.

=== 2. Inspecting Pod Details and Events

For more detailed information, especially when a pod is `Pending` or `CrashLoopBackOff`, use `oc describe`. This command provides a wealth of information including resource requests/limits, volume mounts, container status, and, most importantly, *events*.

*   To get a detailed description of a specific pod:
    ```bash
    oc describe pod <pod-name>
    ```
    The `Events` section at the bottom of the output is crucial. It often explains why a pod failed to schedule, pull an image, or start a container. Look for error messages related to `FailedScheduling` (lack of resources, node selector issues), `FailedCreatePodSandBox`, `ImagePullBackOff` (image not found, registry authentication), `CrashLoopBackOff` (application crash), etc.

=== 3. Accessing Container Logs

Logs are invaluable for understanding what's happening inside a running or crashing container. They provide direct output from the application running within the container.

*   To view logs for a specific container within a pod:
    ```bash
    oc logs <pod-name> -c <container-name>
    ```
    If a pod has only one container, you can typically omit `-c <container-name>`:
    ```bash
    oc logs <pod-name>
    ```
*   To view logs from a *previous* instance of a crashing container:
    ```bash
    oc logs <pod-name> --previous
    ```
    This is extremely useful for `CrashLoopBackOff` scenarios, as the current instance might not have generated sufficient logs before crashing.
*   To follow logs in real-time (similar to `tail -f`):
    ```bash
    oc logs -f <pod-name>
    ```
    You can combine this with `--previous` if needed to start following from the last completed container instance.

[[hands-on-lab-troubleshooting]]
== Hands-on Lab: Troubleshooting a Failing Pod

In this lab, you will simulate a common pod issue and use the `oc` CLI to diagnose and understand it.

=== Scenario: Simulating a Crashing Application

Let's deploy a simple application that is intentionally configured to crash upon startup. This will allow us to observe a pod in a `CrashLoopBackOff` state and practice using troubleshooting commands.

.  **Deploy a failing application:**
    Create a file named `crashing-app.yaml` with the following content. This deployment will create a pod using the `alpine/git` image, but with a command that deliberately exits with an error after a few seconds, simulating an application crash.

    ```yaml
    # crashing-app.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: crashing-web-app
      labels:
        app: crashing-web-app
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: crashing-web-app
      template:
        metadata:
          labels:
            app: crashing-web-app
        spec:
          containers:
          - name: web
            image: alpine/git # A non-web server image to cause an immediate crash
            command: ["sh", "-c", "echo 'Application starting...' && sleep 5 && echo 'Application encountered an error and exiting.' && exit 1"] # Exits with error
            ports:
            - containerPort: 8080
    ```

.  **Apply the deployment to your ARO cluster:**
    Ensure you are logged into your ARO cluster using the `oc login` command.
    ```bash
    oc apply -f crashing-app.yaml
    ```
    *Expected Output:*
    ```
    deployment.apps/crashing-web-app created
    ```

=== Task 1: Observe the Pod's State

.  **Check the status of the deployed pod:**
    ```bash
    oc get pods -l app=crashing-web-app
    ```
    Initially, the pod might be in `ContainerCreating` or `Running` for a brief moment, but it will quickly transition into a `CrashLoopBackOff` state. Observe the `RESTARTS` count increasing over time.

    *Expected Output (similar to after a few moments):*
    ```
    NAME                             READY   STATUS             RESTARTS   AGE
    crashing-web-app-xxxxxxxx-xxxxxx   0/1     CrashLoopBackOff   1          10s
    ```
    Wait a few more seconds and run the command again to see the `RESTARTS` count increment.

=== Task 2: Inspect Pod Details and Events

.  **Get detailed information about the failing pod:**
    Replace `<pod-name>` with the actual name of your crashing pod (e.g., `crashing-web-app-7b8c9d0e-f1g2h`).
    ```bash
    oc describe pod <pod-name>
    ```

.  **Analyze the output:**
    *   Scroll down to the `Events` section. What do you see? Look for messages indicating the container crashed, such as `Back-off restarting failed container`.
    *   Under the `Containers` section, find the `web` container. Observe its `State`. You should see `Terminated` with `Reason: Error` and an `Exit Code: 1`, confirming the intentional crash as per our `command` in the YAML.

=== Task 3: Review Container Logs

.  **View the logs for the crashing container:**
    ```bash
    oc logs <pod-name>
    ```
    *Expected Output (from the last attempt to run the container):*
    ```
    Application starting...
    Application encountered an error and exiting.
    ```
    This output clearly shows the container's execution before it exited.

.  **View previous logs (if the pod restarted multiple times):**
    ```bash
    oc logs <pod-name> --previous
    ```
    This command is crucial when a container repeatedly crashes. It allows you to see the logs from the *previous* failed instance, which might contain critical information that isn't available from the currently restarting instance.

=== Task 4: Clean Up

.  **Delete the deployment to remove the crashing application:**
    ```bash
    oc delete -f crashing-app.yaml
    ```
    *Expected Output:*
    ```
    deployment.apps "crashing-web-app" deleted
    ```
    This command will also delete the associated pod and any other resources created by the deployment.

[[advanced-troubleshooting-monitoring]]
== Advanced Troubleshooting with Monitoring and Logging

While `oc logs` and `oc describe` are powerful for immediate diagnosis, comprehensive troubleshooting, especially in production environments, relies heavily on integrated monitoring and logging solutions. These tools provide historical data, aggregated views, and advanced analytics that go beyond simple CLI commands.

.Ensuring Logging Solution Deployment
[NOTE]
====
For robust cluster management, it's vital to ensure your logging infrastructure is correctly deployed and functioning. The provided context highlights the importance of verifying such solutions. While the specific commands like `kubectl get ds ama-logs-windows --namespace=kube-system` and `kubectl get deployment ama-logs-rs --namespace=kube-system` are used to verify the deployment of the `omsagent` (Container insights agent) in Azure Kubernetes Service (AKS), the underlying principle of confirming your logging agents are running correctly applies universally across Kubernetes distributions, including Azure Red Hat OpenShift. You would typically verify OpenShift Logging (Fluentd, Elasticsearch/OpenSearch) components similarly for ARO.
====

OpenShift clusters typically integrate with logging solutions like OpenShift Logging (often powered by Fluentd for log collection, Elasticsearch or OpenSearch for storage, and Kibana or OpenSearch Dashboards for visualization) for collecting, storing, and visualizing cluster-wide logs. Azure Red Hat OpenShift also integrates with Azure Monitor for Containers (Container insights) for performance and health monitoring.

=== Analyzing Monitoring Data

*   **Azure Portal Container insights:** Learn how to xref:https://docs.microsoft.com/azure/azure-monitor/containers/container-insights-overview[Analyze Kubernetes monitoring data in the Azure portal Container insights] to gain insights into pod and node performance, resource utilization, and health. This can help identify resource starvation, abnormal behavior, or bottlenecks leading to pod issues.
*   **OpenShift Web Console:** The OpenShift Web Console provides comprehensive dashboards for monitoring resource usage, pod health, and application metrics. Navigate to *Monitoring* -> *Metrics* or *Dashboards* to visualize performance data, create custom queries, and track trends over time.
*   **Alerting Systems:** Configure alerts based on predefined conditions like high CPU/memory utilization, frequent pod restarts, deployment failures, or error rates in application logs. Proactive alerts can notify you of potential issues before they impact users.

[[troubleshooting-guide]]
=== Referencing Troubleshooting Guides

As noted in the general troubleshooting advice provided in the context, xref:https://docs.microsoft.com/azure/openshift/openshift-faq#troubleshooting[if you experience issues attempting to onboard or operate], always refer to official troubleshooting guides. For Azure Red Hat OpenShift, this would typically involve documentation from Microsoft Azure and Red Hat OpenShift, which provide specific guidance for common platform-level issues and best practices.

[[best-practices]]
== Best Practices for Troubleshooting

*   **Start with the basics:** Always begin with `oc get pods` and `oc describe pod` to get a quick overview and initial clues.
*   **Check logs first:** Container logs usually contain the most direct explanation of what went wrong within the application itself.
*   **Isolate the problem:** Determine if the issue is with the application code, the container image, the Kubernetes configuration (e.g., resource requests/limits, network policies), or the underlying infrastructure.
*   **Use events:** The `Events` section of `oc describe` is a goldmine for scheduling, image pulling, and container startup issues.
*   **Leverage monitoring tools:** For persistent, intermittent, or hard-to-diagnose issues, monitoring dashboards and historical data are essential for identifying patterns and root causes.
*   **Divide and conquer:** If an application consists of multiple interconnected components (pods, services), troubleshoot them one by one, starting from the dependencies.
*   **Verify external dependencies:** Ensure databases, external APIs, persistent storage, and other services are reachable and functional. Network connectivity issues are a common cause of application problems.
*   **Check resource limits:** Ensure that containers have appropriate CPU and memory limits. Under-provisioning can lead to `OOMKilled` or throttled applications, while over-provisioning can waste resources.

This concludes the module on troubleshooting containers and pods.