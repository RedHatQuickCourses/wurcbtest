#  Configure log forwarding

= Configure Log Forwarding in Azure Red Hat OpenShift

Log forwarding is a critical component of any robust monitoring and troubleshooting strategy for applications and infrastructure. In Azure Red Hat OpenShift (ARO), where the control plane and underlying infrastructure are jointly managed by Microsoft and Red Hat, understanding how to effectively collect, filter, and forward logs to external systems is essential for gaining deep insights into your cluster's health and application behavior.

== Understanding Log Forwarding in ARO

ARO leverages the powerful logging capabilities inherent in OpenShift, primarily through the OpenShift Logging stack. This stack provides a centralized logging solution for collecting various types of logs generated within the cluster and can be configured to forward them to external destinations.

=== Why Forward Logs?

While ARO provides basic monitoring capabilities, forwarding logs to external systems offers several advantages:

*   *Long-term Storage and Retention:* External systems like Azure Log Analytics or dedicated SIEM solutions can offer more extensive and cost-effective log retention policies than internal cluster storage.
*   *Advanced Analytics and Correlation:* Integrating with external tools allows for sophisticated querying, dashboarding, alerting, and correlation of logs with other operational data.
*   *Compliance and Auditing:* Many compliance standards require logs to be stored securely and redundantly outside the primary application environment.
*   *Separation of Concerns:* Keeping operational logs separate from the cluster allows for easier troubleshooting even if the cluster itself is experiencing issues.
*   *Unified Monitoring:* For organizations with multiple clusters or hybrid cloud environments, forwarding logs to a central system provides a single pane of glass for all operational data.

=== OpenShift Logging Stack Overview

The OpenShift Logging stack typically comprises the following components:

*   **Cluster Logging Operator:** This operator manages the deployment and lifecycle of the logging components within the cluster.
*   **Fluentd (Collector):** Deployed as a DaemonSet on each node, Fluentd collects logs from various sources (application containers, OpenShift components, node system logs).
*   **Elasticsearch (Aggregator/Storage):** Historically used for storing collected logs. While `Elasticsearch` is often part of the traditional EFK stack, `Loki` (for Prometheus-style log querying) or external solutions are increasingly common for log storage in modern OpenShift deployments, especially when forwarding.
*   **Kibana (UI):** Provides a web interface for searching and visualizing logs stored in Elasticsearch.

In ARO, while the control plane is managed, you retain full control over configuring the logging stack for your application and infrastructure logs generated on the worker nodes.

=== Types of Logs Collected

The OpenShift Logging stack can collect logs from three main sources:

*   **Application Logs:** Logs generated by your applications running in pods. These are typically read from `stdout` and `stderr` streams of containers.
*   **Infrastructure Logs:** Logs generated by OpenShift internal components, such as the API server, controller manager, scheduler, and other platform services.
*   **Audit Logs:** Logs detailing API requests and actions performed against the Kubernetes API, providing a crucial record for security and compliance.

=== External Log Destinations

ARO supports forwarding logs to various external destinations. Common choices include:

*   **Azure Log Analytics:** A powerful monitoring solution within Azure that can collect data from various sources, including OpenShift. It offers advanced query capabilities and integration with Azure Monitor.
*   **Azure Event Hubs:** A highly scalable data streaming platform that can be used as an intermediary to stream logs to other processing or storage systems.
*   **Splunk:** A widely used SIEM (Security Information and Event Management) platform for log aggregation, analysis, and security monitoring.
*   **Syslog:** A standard protocol for sending log messages to a central log server.
*   **HTTP/HTTPS Endpoints:** Generic endpoints for custom integrations.

The choice of destination often depends on your organization's existing logging infrastructure, compliance requirements, and desired analytics capabilities.

=== Configuring Log Forwarding with `ClusterLogForwarder`

The primary mechanism for configuring log forwarding in OpenShift (and thus ARO) is through the `ClusterLogForwarder` custom resource (CR). This CR allows you to define:

*   **Inputs:** Which types of logs to collect (application, infrastructure, audit).
*   **Outputs:** The external destinations where logs should be sent (e.g., Azure Log Analytics, Event Hubs, Syslog).
*   **Pipelines:** Rules that connect inputs to outputs, potentially with filtering or transformation steps in between.

This declarative approach ensures that your log forwarding configuration is managed as code and can be version-controlled.

IMPORTANT: Before configuring log forwarding, ensure you have an appropriate external log destination set up and accessible from your ARO cluster. This might involve setting up an Azure Log Analytics Workspace, an Azure Event Hub, or a syslog server, and configuring network egress rules if necessary. Refer to the ARO support policies documentation for information on outbound traffic and egress requirements.

== Hands-on Activity: Configuring Basic Log Forwarding in ARO

This activity demonstrates how to deploy the OpenShift Logging Operator and configure a `ClusterLogForwarder` to send application logs to a hypothetical external HTTP endpoint. For a real-world scenario, you would replace the HTTP output with a configured Azure Log Analytics or Event Hub output.

=== Prerequisites

*   An existing Azure Red Hat OpenShift cluster.
*   `oc` CLI tool configured and logged in to your ARO cluster with `cluster-admin` privileges.

=== Step 1: Install the OpenShift Logging Operator

First, you need to install the OpenShift Logging Operator, which manages the logging components.

1.  **Log in to the OpenShift Web Console** as a user with `cluster-admin` privileges.
2.  Navigate to **Operators** -> **OperatorHub**.
3.  Search for `Red Hat OpenShift Logging` and select it.
4.  Click **Install**.
5.  On the "Install Operator" page:
    *   Set the "Installation mode" to `All namespaces on the cluster (default)`.
    *   Set the "Installed Namespace" to `openshift-logging`.
    *   Set the "Update channel" to `stable`.
    *   Set the "Approval strategy" to `Automatic`.
6.  Click **Install**.
7.  Wait for the operator to become `Succeeded` in the `openshift-logging` namespace. You can monitor its status from **Operators** -> **Installed Operators**.

Alternatively, using the `oc` CLI:

.Create the OperatorGroup:
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-logging-og
  namespace: openshift-logging
spec:
  targetNamespaces:
  - openshift-logging
EOF
----

.Subscribe to the Logging Operator:
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-logging
  namespace: openshift-logging
spec:
  channel: "stable"
  name: cluster-logging
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

.Verify the Operator is running:
[source,bash]
----
oc get pod -n openshift-logging -l name=cluster-logging-operator
----
Wait until the pod shows `Running` status.

=== Step 2: Create a `ClusterLogging` Instance

The `ClusterLogging` custom resource defines the overall logging stack. For log forwarding, we primarily need the `logForwarding` component.

1.  **Create a `ClusterLogging` instance:**
    [source,bash]
    ----
    cat <<EOF | oc apply -f -
    apiVersion: "logging.openshift.io/v1"
    kind: "ClusterLogging"
    metadata:
      name: "instance"
      namespace: "openshift-logging"
    spec:
      managementState: "Managed"
      logStore:
        type: "none" # We are only forwarding, not storing internally
      collection:
        logs:
          type: "fluentd"
    EOF
    ----
    *   `logStore: type: "none"`: This tells the operator not to deploy an internal log store (like Elasticsearch), as we are primarily interested in forwarding logs externally.
    *   `collection: logs: type: "fluentd"`: This ensures Fluentd is deployed to collect logs.

2.  **Verify the `ClusterLogging` instance and Fluentd deployment:**
    [source,bash]
    ----
    oc get ClusterLogging -n openshift-logging
    oc get pod -n openshift-logging -l component=collector
    ----
    You should see a `ClusterLogging` instance named `instance` and Fluentd collector pods running on your worker nodes.

=== Step 3: Define an External Log Output (Example: Generic HTTP Endpoint)

For this example, we'll simulate forwarding to a generic HTTP endpoint. In a real ARO scenario, you would configure an output for Azure Log Analytics or Event Hubs.

First, let's create a placeholder `Secret` for credentials if the output required them (e.g., an API key or shared secret for Azure Log Analytics). For a simple HTTP post, it might not be strictly needed, but it's good practice.

1.  **Create a dummy secret (if needed for a real output):**
    [source,bash]
    ----
    oc create secret generic log-forwarding-secrets --from-literal=endpoint-url='http://my-dummy-log-receiver.example.com/api/logs' --from-literal=api-key='dummy-api-key' -n openshift-logging
    ----
    NOTE: Replace `http://my-dummy-log-receiver.example.com/api/logs` with your actual log receiver URL. For Azure Log Analytics, this would involve workspace ID and shared key.

2.  **Create a `ClusterLogForwarder` custom resource:**
    This CR defines where logs should go. We'll set up a pipeline to send application logs.

    [source,bash]
    ----
    cat <<EOF | oc apply -f -
    apiVersion: "logging.openshift.io/v1"
    kind: "ClusterLogForwarder"
    metadata:
      name: "instance"
      namespace: "openshift-logging"
    spec:
      outputs:
        - name: "my-external-http-endpoint"
          type: "http"
          url: "https://your-log-analytics-workspace-id.ods.opinsights.azure.com/api/logs?api-version=2016-04-01" # Replace with actual URL
          secret:
            name: "log-forwarding-secrets" # Refers to the secret created above
            key: "endpoint-url"
          headers:
            - name: "Authorization"
              value: "SharedKey YOUR_SHARED_KEY_BASE64_ENCODED" # Replace with actual auth header, typically generated for Azure Log Analytics
            - name: "Content-Type"
              value: "application/json"
            - name: "x-ms-date"
              value: "$(date -u +'%a, %d %b %Y %H:%M:%S GMT')" # Example dynamic header for Azure Log Analytics
      pipelines:
        - name: "forward-app-logs-to-external"
          inputRefs:
            - "application" # Collects all application logs
          outputRefs:
            - "my-external-http-endpoint"
          # Optional: Add filters here
          # filters:
          #   - type: "k8s_container_selector"
          #     k8sContainerSelector:
          #       containerName: "my-app-container"
          #       namespace: "my-app-namespace"
    EOF
    ----

    *   **For Azure Log Analytics (Example `outputs` configuration):**
        *   You would need the Workspace ID and the Shared Key. The Shared Key needs to be base64 encoded.
        *   The `url` would be specific to your workspace.
        *   The `headers` would include `Authorization` (SharedKey) and `x-ms-date`.
        *   The `secret` would store the Workspace ID and Shared Key.

    [source,yaml]
    ----
    # Example for Azure Log Analytics (Conceptual - details depend on specific integration method)
    # This might require a custom Fluentd plugin or more advanced configuration,
    # or using Azure Event Hubs as an intermediary.
    # Always consult official Azure/Red Hat documentation for definitive ARO Log Analytics integration.

    # Outputs for Azure Log Analytics (conceptual)
    # This would typically involve a dedicated Fluentd plugin or integration via Event Hubs.
    # The direct HTTP output might not be robust enough for all Azure Log Analytics scenarios.
    outputs:
      - name: "azure-log-analytics"
        type: "syslog" # Or http, depending on plugin/method
        # url: "..." # For HTTP
        # syslog:
        #   tag: "app"
        #   hostname: "log-analytics-endpoint" # Specific Azure endpoint
        #   port: 514
        #   protocol: "tcp" # Or tls
        # secret:
        #   name: "azure-log-analytics-secret" # Secret containing Workspace ID, Shared Key
    ----

    NOTE: The HTTP output shown above is a simplified example. Real-world integration with Azure Log Analytics typically involves specific authentication headers, payload formats, and potentially custom Fluentd configurations or using Azure Event Hubs as an intermediary for robust, scalable log ingestion. Always refer to the latest Azure Red Hat OpenShift and Azure Log Analytics documentation for the most accurate and secure integration patterns.

=== Step 4: Verify Log Forwarding

1.  **Check `ClusterLogForwarder` status:**
    [source,bash]
    ----
    oc get ClusterLogForwarder -n openshift-logging instance -o yaml
    ----
    Look for `conditions` indicating `LogForwarding` is `True` and healthy.

2.  **Generate some application logs:**
    Deploy a simple application that continuously generates logs.
    [source,bash]
    ----
    oc new-project test-logging
    oc new-app --docker-image=centos/s2i-nodejs:14-ubi8~https://github.com/sclorg/nodejs-ex.git --name=log-generator
    ----
    After the pod is running, it will generate "Hello world!" messages to `stdout`.

3.  **Monitor the Fluentd logs:**
    You can check the Fluentd collector logs for indications that it's processing and forwarding messages.
    [source,bash]
    ----
    oc logs -f -n openshift-logging -l component=collector
    ----
    You should see Fluentd processing application logs and attempting to send them to the configured output. If your external HTTP endpoint is functional, you should see the logs appear there.

This activity provides a foundational understanding of how to set up log forwarding in ARO. For production environments, further considerations include robust authentication, network security, log filtering, and data transformation to meet specific requirements for your chosen external logging solution.