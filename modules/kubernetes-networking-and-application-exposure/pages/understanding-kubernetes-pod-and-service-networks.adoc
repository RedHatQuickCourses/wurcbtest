#  Understanding Kubernetes Pod and Service Networks

= Understanding Kubernetes Pod and Service Networks

In Azure Red Hat OpenShift (ARO), a robust and flexible networking model is fundamental for application communication, scalability, and external access. Kubernetes provides a sophisticated abstraction layer over the underlying network infrastructure, simplifying how containers discover and communicate with each other and with the outside world. This section delves into the core concepts of Pod and Service networks, which are the backbone of application connectivity within an ARO cluster.

== The Foundation: Addressing Container Networking Challenges

Containers are designed to be agile and ephemeral, often starting, stopping, and relocating across different nodes within a cluster. This dynamic nature poses a significant challenge for traditional networking models, which typically rely on static IP addresses and rigid configurations. Kubernetes addresses these challenges by defining a clear and consistent networking model that ensures:

*   Every Pod gets its own unique IP address.
*   Pods can communicate with each other regardless of which node they reside on.
*   Applications can be reliably accessed through stable network endpoints, even as underlying Pods change.
*   External traffic can be efficiently routed to applications running inside the cluster.

This model is critical for microservices architectures, where many independent services need to communicate seamlessly.

== Kubernetes Pod Networks

At its core, Kubernetes mandates that every Pod is assigned its own unique IP address within a flat, shared network space. This principle is crucial for simplicity and interoperability, as it means:

*   **Unique IP per Pod**: Just like a physical host or virtual machine, each Pod receives a distinct IP address. This IP is stable for the lifetime of the Pod, but changes if the Pod is restarted or rescheduled.
*   **Flat Network**: All Pods in the cluster can communicate with each other directly using their Pod IPs, without the need for Network Address Translation (NAT) between Pods. This allows applications to see the true IP address of their communicating peer.
*   **Node-Agnostic Communication**: Pods can communicate whether they are on the same worker node or on different worker nodes. The underlying Container Network Interface (CNI) plugin handles the routing and ensures seamless connectivity.

In Azure Red Hat OpenShift, the default CNI plugin is xref:../../cluster-configuration/ovn-kubernetes.adoc[OVN-Kubernetes]. As mentioned in the `CONTEXT`, ARO "Install[s], configure[s], and maintain[s] the OVN-Kubernetes network plugin and related components for default internal pod traffic." OVN-Kubernetes manages the IP address allocation for Pods and sets up the necessary network overlays to allow Pods on different nodes to communicate directly.

=== How Pod IPs Are Allocated

When a Pod is scheduled onto a worker node, the CNI plugin on that node is responsible for allocating an IP address to the Pod. This IP address is drawn from a larger *Pod CIDR* range defined for the entire cluster (e.g., `10.128.0.0/14`). Each worker node is then allocated a smaller sub-range from this overall Pod CIDR (e.g., `/23`), and Pods on that specific node receive their IPs from its allocated sub-range.

This design ensures that every Pod has a unique IP address across the entire cluster, making inter-Pod communication straightforward. While the core Pod networking is managed by ARO, you can xref:network-security-policies.adoc[configure NetworkPolicy objects] to control ingress and egress traffic for specific Pods and projects, enforcing fine-grained security rules within this flat network.

[NOTE]
====
In a managed service like ARO, the underlying network infrastructure components, including the `OVN-Kubernetes` plugin and core IP address ranges (`machine CIDR`, `service CIDR`, and `pod CIDR`), are expertly managed by the joint Microsoft and Red Hat operations team. While you can request optional non-default IP address ranges during cluster provisioning, the day-to-day configuration and maintenance of the core network fabric are handled for you.
====

== Kubernetes Service Networks

While Pods are assigned unique IP addresses, these IPs are inherently ephemeral. Pods can be created, destroyed, or scaled up and down, leading to changing IP addresses. For other applications or external clients to reliably access a set of Pods, a stable and persistent network endpoint is required. This is where Kubernetes Services come into play.

A Kubernetes Service is an abstract way to expose an application running on a set of Pods as a network service. It provides a stable IP address (known as a ClusterIP) and a DNS name that remains constant, even if the underlying Pods change, scale, or move to different nodes.

=== Key Characteristics of Services

*   **Stable IP and DNS**: Services offer a persistent ClusterIP and a corresponding DNS name, decoupling client access from the ephemeral nature of Pod IPs.
*   **Load Balancing**: A Service acts as a load balancer, distributing incoming network traffic across the healthy Pods that match its selector.
*   **Service Discovery**: Other Pods or external entities can discover and communicate with an application simply by addressing its Service IP or DNS name.
*   **Abstraction**: Services abstract away the details of Pod lifecycle management, presenting a consistent interface to consumers.

=== Types of Services

Kubernetes offers several types of Services to cater to different exposure requirements:

.Service Types
*   **`ClusterIP` (Default)**:
    *   Exposes the Service on an internal IP address within the cluster.
    *   This Service is only reachable from within the cluster, making it ideal for internal microservice communication.
    *   This is the most common Service type for internal application components.

*   **`NodePort`**:
    *   Exposes the Service on a static port on *each* worker node's IP address.
    *   Any traffic sent to `NodeIP:NodePort` is routed to the Service.
    *   Primarily used for testing or in environments where a public cloud load balancer is not desired or easily provisioned.

*   **`LoadBalancer`**:
    *   Exposes the Service externally using a cloud provider's load balancer.
    *   In ARO, this automatically provisions an Azure Load Balancer, which then routes external traffic to the Service's NodePort on the worker nodes.
    *   This is essential for production applications that require public internet access, aligning with the `CONTEXT`'s point "Set up public cloud load balancers" for applications.

*   **`ExternalName`**:
    *   Maps the Service to an arbitrary DNS name by returning a `CNAME` record.
    *   No proxying or load balancing is involved; it's purely a DNS alias.
    *   Useful for integrating with services running outside your Kubernetes cluster (e.g., an external database).

=== How Services Function

The `kube-proxy` component, which runs on every worker node, is responsible for implementing the Service networking model. It watches the Kubernetes API server for changes to Services and their associated Pods. When a Service is created, `kube-proxy` configures `iptables` rules (or IPVS) on the node to:

*   Intercept traffic destined for the Service's ClusterIP.
*   Distribute this traffic to one of the healthy backend Pods that match the Service's selector.

For `LoadBalancer` type Services, the cloud provider's load balancer (e.g., Azure Load Balancer) is configured to direct external traffic to the `NodePort` of the Service on the worker nodes. `kube-proxy` then takes over, forwarding that traffic to the correct Pods.

== OpenShift Ingress for Advanced External Access

While `LoadBalancer` Services are suitable for direct external exposure, OpenShift provides a more advanced and flexible mechanism for HTTP/HTTPS access: the Ingress. The `CONTEXT` highlights that ARO "Set[s] up the OpenShift Ingress cluster operator and the default IngressController."

An OpenShift `Route` (which builds upon Kubernetes `Ingress` resources) allows external clients to access services within the cluster via a DNS-resolvable hostname. Routes leverage the `IngressController`, which acts as a reverse proxy (e.g., based on HAProxy or NGINX) to efficiently manage incoming traffic. IngressControllers are often deployed on dedicated infrastructure nodes (as suggested by the `CONTEXT` for "high ingress resource requirements") to separate their workload from application Pods, thereby routing traffic to the appropriate Service and its backend Pods.

.Relationship between Pods, Services, and Ingress/Routes
*   **Pods**: The actual running instances of your application, with ephemeral internal IPs.
*   **Services**: Provide stable internal IP endpoints and handle load balancing for a set of Pods.
*   **Ingress/Routes**: Offer external HTTP/HTTPS access, often with hostname-based routing, TLS termination, and advanced traffic management features, leveraging Load Balancers or NodePorts implicitly to expose the IngressController itself.

Understanding this hierarchy – from individual Pod IPs to stable Service endpoints and finally to external Ingress/Routes – is key to designing, deploying, and troubleshooting applications within your Azure Red Hat OpenShift environment.

== Hands-on Activity: Exploring Kubernetes Pod and Service Networks

This guided activity will enable you to explore the foundational networking components within an Azure Red Hat OpenShift cluster. You will use the `oc` CLI to inspect Pod IPs, Service configurations, and observe how these elements contribute to application connectivity.

=== Objectives

*   Identify the unique IP addresses assigned to Pods.
*   Examine various Service types and their configurations.
*   Understand how Services abstract the ephemeral nature of Pods.
*   Trace the flow of traffic from a Service to its backend Pods.

=== Prerequisites

*   An active Azure Red Hat OpenShift cluster.
*   `oc` CLI configured and authenticated to your cluster.
*   At least one application deployed in a project (e.g., a sample application or one of the default OpenShift components).

=== Steps

.  **Access your ARO Cluster**
    Ensure you are logged into your ARO cluster using the `oc` CLI. Replace the placeholders with your actual token and API server URL.
+
[,bash]
----
oc login --token=<your_token> --server=<your_api_server_url>
----
+
*Expected Output*: A message confirming successful login to your cluster.

.  **List All Pods and Their IP Addresses**
    Use the `oc get pods` command with the `-A` (all namespaces) and `-o wide` flags to display detailed information about all Pods, including their assigned IP addresses and the worker node they are running on.
+
[,bash]
----
oc get pods -A -o wide
----
+
*Observe*:
*   Each Pod has a unique `IP` address.
*   Pods running on the same `NODE` will have different `IP` addresses.
*   Note the `IP` addresses of a few Pods; these are their internal Pod network IPs.

.  **Examine Services in a Project**
    Choose a project where applications are deployed (e.g., `openshift-ingress`, `default`, or one of your custom application projects). List the Services within that project. For this example, we'll look at the `openshift-ingress` project.
+
[,bash]
----
oc get svc -n openshift-ingress
----
+
*Observe*:
*   Identify the `CLUSTER-IP` for each Service. This is the stable, internal IP address for the Service.
*   Note the `TYPE` column (e.g., `ClusterIP`, `LoadBalancer`).
*   Pay attention to the `PORT(S)` column, showing which ports the Service is listening on.

.  **Inspect a Specific `ClusterIP` Service**
    Pick a `ClusterIP` type Service from the `openshift-ingress` project (e.g., `router-default`) and describe it to see its detailed configuration, including its selector and endpoint Pods.
+
[,bash]
----
oc describe svc router-default -n openshift-ingress
----
+
*Observe*:
*   **`Selector` field**: This defines how the Service identifies its backend Pods (e.g., `router=default`).
*   **`Endpoints` list**: This shows the IP addresses and ports of the actual Pods that are currently backing this Service. These are the Pod IPs you observed in Step 2. Notice how the Service abstracts these ephemeral Pod IPs into a single, stable `CLUSTER-IP`.

.  **Explore a `LoadBalancer` Service (if available)**
    If your cluster hosts an application exposed via a `LoadBalancer` Service, you can inspect it. First, find any `LoadBalancer` type Services:
+
[,bash]
----
oc get svc -A | grep LoadBalancer
----
+
Then, describe one of them. Replace `<loadbalancer_service_name>` and `<project_name>` with the appropriate values.
+
[,bash]
----
oc describe svc <loadbalancer_service_name> -n <project_name>
----
+
*Observe*:
*   Note the `Load Balancer Ingress` field (often labeled `EXTERNAL-IP`). This is the public IP address assigned by Azure Load Balancer, through which your application is accessible from outside the cluster.
*   Confirm that this Service also has `Endpoints` listed, pointing to the internal Pod IPs.

.  **Test Internal Application Access via Service**
    To demonstrate internal communication through a Service, you can launch a temporary debug Pod and use `curl` to access another Service within the cluster.
+
a.  Launch a temporary debug Pod in your current project.
+
[,bash]
----
oc run debug-pod --image=registry.access.redhat.com/ubi8/ubi-minimal --command -- sleep infinity
----
+
b.  Get the name of your newly created debug Pod.
+
[,bash]
----
oc get pod -l run=debug-pod -o jsonpath='{.items[0].metadata.name}'
----
+
c.  Execute `curl` inside the debug Pod to access a `ClusterIP` Service. For example, if you have a Service named `my-app-service` in the `my-project` project listening on port `8080`, you would use:
+
[,bash]
----
oc exec -it <debug-pod-name> -- curl my-app-service.my-project.svc.cluster.local:8080
----
+
*Replace*:
*   `<debug-pod-name>` with the actual name of your debug Pod.
*   `my-app-service` with the name of an actual `ClusterIP` Service in your cluster (e.g., `kubernetes` in the `default` namespace).
*   `my-project` with the project name where the service resides.
*   `8080` with the actual port of the service.
+
*Observe*: You should see the response from the targeted application, confirming that internal Pod-to-Service communication works using the stable DNS name provided by Kubernetes.

.  **Clean up the debug Pod**
    Once you've completed the activity, delete the temporary debug Pod.
+
[,bash]
----
oc delete pod debug-pod
----

This hands-on activity provides a practical demonstration of Kubernetes Pod and Service networks, reinforcing your understanding of how internal communication and external exposure are managed within an Azure Red Hat OpenShift environment.